import pandas as pd
import evaluate as hf_evaluate
from naive_rag import get_or_build_faiss, rag
import config
import numpy as np
import faiss
from enhanced_rag import enhanced_rag
import json, os, random
RESULTS_PATH_STEP3 = "results/naive_results_step3.json"
RESULTS_PATH_STEP4 = "results/naive_results_step4.json"
RESULTS_PATH_STEP5 = "results/enhanced_results_step5.json"

# ---------- Load Test Queries ----------
def load_test_queries(path=config.QUERIES_PATH):
    """Load test dataset and drop missing values."""
    df = pd.read_parquet(path).dropna(subset=["question", "answer"])
    df = df.reset_index(drop=True)
    return df

# ---------- Run Evaluation for One Style ----------
def evaluate(schema, index, embedder, queries, style="basic", k=1, max_queries=None):
    # Run RAG on all queries with a given prompt style.
    # max_queries: int (optional) - used for debugging smaller subsets
   
   # Preds: The answers generated by our RAG pipeline
   # Golds : The right/perfect answer in test data
    preds, golds = [], []

    if max_queries:
        queries = queries.head(max_queries)

    for i, row in queries.iterrows():
        q = row["question"]
        gold = row["answer"]

        concat = True if k > 1 else False

        try:
             # ---Prediction for Naive rag---
             # pred = rag(q, schema, index, embedder, k=k, style=style,concat=concat )

            # ---Prediction for Enhanced rag---
            pred = enhanced_rag(q, schema, index, embedder, k=3)

        except Exception as e:
            pred = "ERROR"

        preds.append(pred)
        golds.append(str(gold))
    
    return preds, golds

# ---------- Compute Metrics ----------
def compute_metrics(preds, golds):
    squad_metric = hf_evaluate.load("squad")
    predictions = [{"id": str(i), "prediction_text": p} for i, p in enumerate(preds)]
    references  = [{"id": str(i), "answers": {"text": [g], "answer_start": [0]}} for i, g in enumerate(golds)]
    scores = squad_metric.compute(predictions=predictions, references=references)
    return scores


# ---------- STEP 4: Evaluating on different embedding size and top-k ----------
def run_step4_experiments():
    # Compare embedding sizes and retrieval depths by rebuilding FAISS with different sentence-transformer models.
    models = {
        256: "sentence-transformers/paraphrase-MiniLM-L3-v2",
        384: "sentence-transformers/all-MiniLM-L6-v2",
        512: "sentence-transformers/all-mpnet-base-v2"
    }
    retrieval_ks = [1, 3, 5]
    results_dict = {}

    queries = load_test_queries().head(100) #Running with 300 queries from the test dataset
    schema = pd.read_parquet(config.PASSAGES_PATH)

    for dim, model_name in models.items():
        print(f"\n>> Building FAISS with model {model_name} ({dim}-dim embeddings)")
        from sentence_transformers import SentenceTransformer
        embedder = SentenceTransformer(model_name)
        embs = embedder.encode(schema["passage"].tolist(), convert_to_numpy=True, show_progress_bar=True)
        embs = embs / np.linalg.norm(embs, axis=1, keepdims=True)

        index = faiss.IndexFlatIP(embs.shape[1])
        index.add(embs.astype("float32"))

        model_results = {}
        for k in retrieval_ks:
            print(f"   Evaluating top_k = {k}")
            preds, golds = evaluate(schema, index, embedder, queries, style="basic", k=k)
            scores = compute_metrics(preds, golds)
            print(f"      EM={scores['exact_match']:.2f}, F1={scores['f1']:.2f}")

            model_results[f"top_{k}"] = {
                "exact_match": scores["exact_match"],
                "f1": scores["f1"]
            }

        results_dict[model_name] = model_results

    # --- Save results ---
    os.makedirs("results", exist_ok=True)
    with open(RESULTS_PATH_STEP4, "w") as f:
        json.dump(results_dict, f, indent=2)

    print(f"Saved results to {RESULTS_PATH_STEP4}")


# ---------- Main ----------

if __name__ == "__main__":

    schema, index, embedder = get_or_build_faiss(config.PASSAGES_PATH, config.EMBED_MODEL)
    queries = load_test_queries()

    all_results = {}
    # Change here based on what type of styles to run and evaluate
    for style in ["basic", "cot", "persona", "instruction"]:
        print(f"\n--- Evaluating Prompting Startegies/Style: {style} ---")

        # compute metrics
        preds, golds = evaluate(schema, index, embedder, queries, style=style, k=1, max_queries=300)
        scores = compute_metrics(preds, golds)
        print(scores)
        all_results[style] = scores

        # ---- Print 2â€“3 random examples for manual inspection ----
        print("\nSample predictions for style:", style)
        sample_idx = random.sample(range(len(preds)), min(3, len(preds)))
        for i in sample_idx:
            print(f"Q: {queries.iloc[i]['question']}")
            print(f"Gold: {golds[i]}")
            print(f"Pred: {preds[i]}")
            print("-" * 40)
    
    # Save results to JSON
    # Make sure to change path as per evaluation step
    os.makedirs("results", exist_ok=True)
    with open(RESULTS_PATH_STEP5, "w") as f:
        json.dump(all_results, f, indent=2)
    
    print(f"Saved results to {RESULTS_PATH_STEP5}")

    # # Run Step 4 only
    # print("\n========== STEP 4: Changing Embedding and Top-k Parameters ==========")
    # run_step4_experiments()
