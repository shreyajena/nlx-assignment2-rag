# src/evaluation.py

import pandas as pd
import evaluate as hf_evaluate
from naive_rag import get_or_build_faiss, rag
import config
import json, os, random
RESULTS_PATH = "results/naive_results.json"

# ---------- Load Test Queries ----------
def load_test_queries(path=config.QUERIES_PATH):
    """Load test dataset and drop missing values."""
    df = pd.read_parquet(path).dropna(subset=["question", "answer"])
    df = df.reset_index(drop=True)
    return df

# ---------- Run Evaluation for One Style ----------
def evaluate(schema, index, embedder, queries, style="basic", k=1, max_queries=None):
    # Run RAG on all queries with a given prompt style.
    # max_queries: int (optional) - used for debugging smaller subsets
   
   # Preds: The answers generated by our RAG pipeline
   # Golds : The right/perfect answer in test data
    preds, golds = [], []

    if max_queries:
        queries = queries.head(max_queries)

    for i, row in queries.iterrows():
        q = row["question"]
        gold = row["answer"]
        
        try:
            pred = rag(q, schema, index, embedder, k=k, style=style)
        except Exception as e:
            pred = "ERROR"

        preds.append(pred)
        golds.append(str(gold))
    
    return preds, golds

# ---------- Compute Metrics ----------
def compute_metrics(preds, golds):
    squad_metric = hf_evaluate.load("squad")
    predictions = [{"id": str(i), "prediction_text": p} for i, p in enumerate(preds)]
    references  = [{"id": str(i), "answers": {"text": [g], "answer_start": [0]}} for i, g in enumerate(golds)]
    scores = squad_metric.compute(predictions=predictions, references=references)
    return scores

# ---------- Main ----------

if __name__ == "__main__":
    schema, index, embedder = get_or_build_faiss(config.PASSAGES_PATH, config.EMBED_MODEL)
    queries = load_test_queries()

    all_results = {}
    for style in ["basic", "cot", "persona", "instruction"]:
        print(f"\n--- Evaluating Prompting Startegies/Style: {style} ---")

        # compute metrics
        preds, golds = evaluate(schema, index, embedder, queries, style=style, k=1, max_queries=300)
        scores = compute_metrics(preds, golds)
        print(scores)
        all_results[style] = scores

        # ---- Print 2â€“3 random examples for manual inspection ----
        print("\nSample predictions for style:", style)
        sample_idx = random.sample(range(len(preds)), min(3, len(preds)))
        for i in sample_idx:
            print(f"Q: {queries.iloc[i]['question']}")
            print(f"Gold: {golds[i]}")
            print(f"Pred: {preds[i]}")
            print("-" * 40)
    
    # Save results to JSON
    os.makedirs("results", exist_ok=True)
    with open(RESULTS_PATH, "w") as f:
        json.dump(all_results, f, indent=2)
    
    print(f"Saved results to {RESULTS_PATH}")